{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dasun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\dasun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\dasun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\dasun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\dasun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\dasun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import mrcnn.config\n",
    "from mrcnn import utils, visualize\n",
    "from mrcnn.model import MaskRCNN\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Configuration that will be used by Mask R-CNN library\n",
    "\n",
    "class MaskRCNNConfig(mrcnn.config.Config):\n",
    "    NAME = \"coco_pretrained_model_config\"\n",
    "    IMAGES_PER_GPU = 1\n",
    "    GPU_COUNT = 1\n",
    "    NUM_CLASSES = 1 + 80  # COCO dataset has 80 classes + one background class\n",
    "    DETECTION_MIN_CONFIDENCE = 0.6\n",
    "    \n",
    "    \n",
    "# Filter the result of Mask R-CNN to only obtain bounding boxes and class names of objects identified as cars\n",
    "\n",
    "def getCarBoxes(boxes,class_ids):\n",
    "    car_boxes = []\n",
    "    \n",
    "    # class_id 3/8 corresponds to car/truck objects as per COCO dataset\n",
    "    for i,box in enumerate(boxes):\n",
    "        if class_ids[i] in [3,8]:\n",
    "            car_boxes.append(boxes)\n",
    "            \n",
    "    return np.array(car_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section_2'>2. Initialize variables and paths</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory of the project\n",
    "ROOT_DIR = \".\"\n",
    "\n",
    "# Local path for logs and trained model.One of the best practices and useful while training the model . \n",
    "# Will not be used in this demonstration\n",
    "MODEL_DIR = os.path.join(ROOT_DIR,\"logs\")\n",
    "\n",
    "\n",
    "# Path for saving trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR,\"mask_rcnn_coco.h5\")\n",
    "\n",
    "\n",
    "# Downloading weights of pre-trained model for COCO dataset from the release \n",
    "# Executed for the first time when to store the model weights in this repo\n",
    "\n",
    "# if not os.path.exists(COCO_MODEL_PATH):\n",
    "#    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section_3'>3. Creating model and loading model weights</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = \"images\"\n",
    "\n",
    "\n",
    "# Create a Mask R-CNN model in inference mode\n",
    "model = MaskRCNN(mode='inference', config=MaskRCNNConfig(),model_dir=MODEL_DIR)\n",
    "\n",
    "\n",
    "# Load pre-trained model, this will load weights of model trained on COCO dataset\n",
    "%time model.load_weights(COCO_MODEL_PATH,by_name=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each object detected in the image fed as input, the model classifies it and returns class ID, which is an integer value identifying each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO Class names\n",
    "# Index of the class in the list is its ID. For example, to get ID of\n",
    "# the person class, use: class_names.index('person')\n",
    "\n",
    "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "               'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section_4'>4. Identifying parking spaces in the parking lot</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bouding box for each car identified is a parking parking spot.\n",
    "\n",
    "These coordinates captured by the system in real time will be used for comparing bounding boxes of subsequent images captured in the system to find if the identified cars's bounding box overlap with the reference spots.\n",
    "\n",
    "If the overlap is above a threshold, the space is occupied, otherwise it is **available** which is our aim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of parking spaces\n",
    "parked_car_boxes = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run instance detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a random image from images folder\n",
    "file_names = next(os.walk(IMAGE_DIR))[2]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying empty spots for overcast conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the first and 3rd image from a sequence of file_names \n",
    "# First image is the reference image of completely occupied parking area. It is used for finding all parking spots\n",
    "\n",
    "image1 = skimage.io.imread(os.path.join(IMAGE_DIR,file_names[0]))\n",
    "image2 = skimage.io.imread(os.path.join(IMAGE_DIR,file_names[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run detection\n",
    "# runs the detection pipeline and returns a list of dictionaries, one dict per image.\n",
    "\n",
    "%time result1 = model.detect([image1])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result2 = model.detect([image2])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = result1[0]\n",
    "r2 = result2[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the result of instance segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.display_instances(image1,r1['rois'],r1['masks'], r1['class_ids'], class_names, r1['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.display_instances(image2,r2['rois'], r2['masks'], r2['class_ids'], class_names, r2['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the results to only get identified cars' bounding boxes\n",
    "\n",
    "car_boxes1 = getCarBoxes(r1['rois'],r1['class_ids'])\n",
    "car_boxes2 = getCarBoxes(r2['rois'],r2['class_ids'])\n",
    "\n",
    "# car_boxes1 and car_boxes2 is an array of length of no. of cars identified with each row having set of \n",
    "# 4 coordinates y1, x1, y2, x2 ; points 1 and 2 are opposite vertices of the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_boxes1[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first image has the parking lot occupied in full capacity. Hence, treating the bounding boxes for these cars as parking lot spaces in subsequent images to identify if these spaces are occupied by cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_spaces = car_boxes1[0]\n",
    "\n",
    "# computinig center locations of each spot\n",
    "center_locs = []\n",
    "\n",
    "for spot_coords in parking_spaces:\n",
    "    center_locs.append([int((spot_coords[1]+spot_coords[3])/2), int((spot_coords[0]+spot_coords[2])/2)])\n",
    "\n",
    "\n",
    "centers = np.array(center_locs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw each box on the frame\n",
    "\n",
    "for i,box in enumerate(parking_spaces):\n",
    "    y1, x1, y2, x2 = box\n",
    "    cv2.rectangle(image1, (x1, y1), (x2, y2),(0,0,255),2)\n",
    "    cv2.circle(image1,(centers[i][0],centers[i][1]),2,(0,0,255),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the image on screen\n",
    "\n",
    "cv2.imshow('image', image1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above snippet gives the following output for identifying all the parking spots\n",
    "\n",
    "<img src='result_images/all_parking_spots.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding box and center location for an occupied spot is displayed in red. Since the location of a parking is the location of the parked car, bounding boxes and center location of all the cars are red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parking_spaces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_boxes2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section_5'>5. Detecting empty parking spots</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much car overlaps with the bounding boxes of parking spaces\n",
    "\n",
    "overlaps = mrcnn.utils.compute_overlaps(car_boxes2[0],parking_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_prob = overlaps.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,box in enumerate(parking_spaces):\n",
    "    y1, x1, y2, x2 = box\n",
    "    \n",
    "    if overlap_prob[i] >= 0.5:\n",
    "        occupancy_status = (0,0,255)\n",
    "    \n",
    "    else:\n",
    "        occupancy_status = (0,255,0)\n",
    "    cv2.rectangle(image2,(x1,y1), (x2,y2) , occupancy_status ,1)\n",
    "    cv2.circle(image2,(centers[i][0],centers[i][1]),2,occupancy_status,2)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('image', image2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above snippet give the following output for identifying empty parking spots in an image captured on a **overcast day**\n",
    "    \n",
    "<img src='result_images/detect_empty_space_overcast.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding box and center location for an **empty spot** is displayed in **green**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
